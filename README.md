# Gender-Occupation-Bias A-MEMIT-based-Approach

AI and Large Language Models (LLMs)
have achieved significant progress in the
last years, yet their increasing complex-
ity has made them difficult to interpret and
prone to perpetuating social biases present
in their training data. The more these mod-
els become complex, the more difficult it is
to provide interpretability and address eth-
ical concerns to produce fairer solutions.
In particular, gender stereotypes are gener-
ally very prominent in occupations. How-
ever, to mitigate the cost of training such
big systems, we would like to identify and
editing the internal representations of the
model that associate occupations with spe-
cific genders, or any other bias in general.
Naturally, this task must be done without
worsening the actual performance of the
model, nor should change what the model
knows about the professions themselves.
